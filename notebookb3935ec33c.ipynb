{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"lucadiliello/hotpotqa\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download('punkt')\n\ndef chunk_text(text, chunk_size=150):\n    \"\"\"Split text into chunks of tokenized sentences with max chunk size.\"\"\"\n    sentences = sent_tokenize(text)\n    chunks = []\n    current_chunk = []\n    current_length = 0\n\n    for sentence in sentences:\n        sentence_length = len(sentence.split())\n        if current_length + sentence_length <= chunk_size:\n            current_chunk.append(sentence)\n            current_length += sentence_length\n        else:\n            chunks.append(\" \".join(current_chunk))\n            current_chunk = [sentence]\n            current_length = sentence_length\n\n    if current_chunk:\n        chunks.append(\" \".join(current_chunk))\n\n    return chunks\n\ndef preprocess_dataset(dataset, chunk_size=150):\n    \"\"\"Preprocess the lucadiliello/hotpotqa dataset by chunking the context passages.\"\"\"\n    processed_data = []\n\n    for item in dataset:\n        question = item['question']  # Adjust field based on the dataset\n        passages = \" \".join(item['context'])  # Combine multiple context passages into one block of text\n\n        # Chunk the combined context text into smaller chunks\n        chunked_passages = chunk_text(passages, chunk_size)\n\n        processed_data.append({\n            'question': question,\n            'chunked_passages': chunked_passages,\n            'answers': item['answers']  \n        })\n\n    return processed_data\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==4.42.4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\nclass MultiStageRetrieval:\n    def __init__(self, embed_model_small, embed_model_large, rank_model_small, rank_model_large):\n        # Embedding Models (Small and Large)\n        self.embed_model_small = SentenceTransformer(embed_model_small)\n\n        self.embed_model_large = NVEmbedModel.from_pretrained(embed_model_large, trust_remote_code=True)\n\n\n        # Ranking Models (Small and Large)\n        self.rank_model_small = AutoModelForSequenceClassification.from_pretrained(rank_model_small)\n        self.rank_tokenizer_small = AutoTokenizer.from_pretrained(rank_model_small)\n        self.rank_model_large = AutoModelForSequenceClassification.from_pretrained(rank_model_large)\n        self.rank_tokenizer_large = AutoTokenizer.from_pretrained(rank_model_large)\n\n    def retrieve_candidates(self, query, chunked_passages, top_k=5, model_type='small'):\n        \"\"\"\n        Retrieve top-k relevant chunked passages using embedding models.\n        Args:\n            query: The input question/query from the dataset.\n            chunked_passages: A list of text chunks from the context passages.\n            top_k: The number of top-k passages to retrieve.\n            model_type: Choose between 'small' or 'large' embedding models.\n        \"\"\"\n        if model_type == 'small':\n            query_embedding = self.embed_model_small.encode(query, convert_to_tensor=True)\n            passage_embeddings = self.embed_model_small.encode(chunked_passages, convert_to_tensor=True)\n        else:\n            query_embedding = self.embed_model_large.encode(query, convert_to_tensor=True)\n            passage_embeddings = self.embed_model_large.encode(chunked_passages, convert_to_tensor=True)\n\n        # Retrieve top-k most similar passages\n        scores = util.pytorch_cos_sim(query_embedding, passage_embeddings)[0]\n        top_k_scores = torch.topk(scores, k=top_k)\n        return top_k_scores.indices, top_k_scores.values\n\n    def rerank(self, query, retrieved_passages, model_type='small'):\n        \"\"\"\n        Rerank the retrieved passages using ranking models.\n        Args:\n            query: The input question/query.\n            retrieved_passages: Passages retrieved from the candidate retrieval stage.\n            model_type: Choose between 'small' or 'large' ranking models.\n        \"\"\"\n        if model_type == 'small':\n            model = self.rank_model_small\n            tokenizer = self.rank_tokenizer_small\n        else:\n            model = self.rank_model_large\n            tokenizer = self.rank_tokenizer_large\n\n        # Prepare the inputs for the ranking model\n        inputs = [tokenizer(query, passage, return_tensors='pt', truncation=True, padding=True) for passage in retrieved_passages]\n        relevance_scores = []\n\n        for input in inputs:\n            with torch.no_grad():\n                outputs = model(**input)\n                relevance_scores.append(outputs.logits[0].item())  # Assumes single logit relevance score\n\n        # Sort the passages by relevance score in descending order\n        ranked_indices = sorted(range(len(relevance_scores)), key=lambda i: relevance_scores[i], reverse=True)\n        return ranked_indices, relevance_scores\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Define the Recall@K computation function\ndef compute_recall_at_k(retrieved_docs, relevant_docs, k=10):\n    \"\"\"Computes Recall@K for the retrieved documents.\"\"\"\n    retrieved_top_k = retrieved_docs[:k]  # Get the top K retrieved documents\n    num_relevant_in_k = len(set(retrieved_top_k) & set(relevant_docs))  # Intersection of relevant and retrieved docs\n\n    # Recall@K is the number of relevant docs in top K divided by total relevant docs\n    recall = num_relevant_in_k / len(relevant_docs) if relevant_docs else 0\n    return recall\n\n# Modify the evaluation function to use batch processing\ndef evaluate_retrieval(retrieval_system, dataset, k=10, batch_size=64):\n    \"\"\"Evaluates the retrieval system using Recall@K with batch processing.\"\"\"\n    total_recall = 0\n    num_samples = len(dataset)\n\n    # Process the dataset in batches\n    for i in range(0, num_samples, batch_size):\n        # Create a batch of items\n        batch = dataset[i:i + batch_size]\n        \n        questions = [item['question'] for item in batch]\n        chunked_passages = [item['chunked_passages'] for item in batch]\n\n        # Retrieve and rerank in batch\n        candidates_batch = retrieval_system.retrieve_batch(questions, chunked_passages)  # Modify retrieval to handle batch\n        reranked_candidates_batch = retrieval_system.rerank_batch(candidates_batch)  # Modify reranking to handle batch\n\n        # Compute Recall@K for each item in the batch\n        for j, item in enumerate(batch):\n            recall_at_k = compute_recall_at_k(reranked_candidates_batch[j], item['answers'], k=k)\n            total_recall += recall_at_k\n\n    # Return the average Recall@K over all samples\n    return total_recall / num_samples\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install einops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the config from the HotpotQA dataset in the  benchmark\nhotpotqa_dataset = load_dataset(\"lucadiliello/hotpotqa\", \"default\", trust_remote_code=True)['train']\n\n# Assuming preprocess_dataset is defined in your notebook\n# Example preprocess function (adjust based on your actual implementation)\n\ndef processed_dataset(dataset, chunk_size=150):\n    \"\"\"Preprocess the lucadiliello/hotpotqa dataset by chunking the context passages.\"\"\"\n    processed_data = []\n\n    for item in dataset:\n        question = item['question']  # Adjust field based on the dataset\n        passages = \" \".join(item['context'])  # Combine multiple context passages into one block of text\n\n        # Chunk the combined context text into smaller chunks\n        chunked_passages = chunk_text(passages, chunk_size)\n\n        processed_data.append({\n            'question': question,\n            'chunked_passages': chunked_passages,\n            'answers': item['answers']  # Add answer for relevance evaluation later\n        })\n    return processed_dataset\n# Preprocess the HotpotQA dataset\nhotpotqa_data = preprocess_dataset(hotpotqa_dataset)\n\n# Assuming MultiStageRetrieval is defined in your notebook\n# Initialize multi-stage retrieval system with selected models\nfrom transformers import AutoModel, AutoTokenizer\n\n# Load the embedding models with trust_remote_code set to True\nembed_model_small = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nembed_model_large = AutoModel.from_pretrained('nvidia/NV-Embed-v2', trust_remote_code=True)\n\n# Load the ranking models with trust_remote_code set to True\nrank_model_small = AutoModel.from_pretrained('cross-encoder/ms-marco-MiniLM-L-12-v2')\nrank_model_large = AutoModel.from_pretrained('nvidia/nv-rerankqa-mistral-4b-v3', trust_remote_code=True)\n\n# Initialize the multi-stage retrieval system with the preloaded models\nretrieval_system = MultiStageRetrieval(\n    embed_model_small=embed_model_small,\n    embed_model_large=embed_model_large,\n    rank_model_small=rank_model_small,\n    rank_model_large=rank_model_large\n)\n\n\"\"\"retrieval_system = MultiStageRetrieval(\n    embed_model_small='sentence-transformers/all-MiniLM-L6-v2',\n    embed_model_large='nvidia/NV-Embed-v2',\n    rank_model_small='cross-encoder/ms-marco-MiniLM-L-12-v2',\n    rank_model_large='nvidia/nv-rerankqa-mistral-4b-v3',\n    trust_remote_code = True\n)\n\"\"\"\navg_recall_hotpotqa = evaluate_retrieval(retrieval_system, hotpotqa_data, k=10)\n\n# Output the evaluation result\nprint(f'Average Recall@10 on HotpotQA: {avg_recall_hotpotqa:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}